{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This note book runs the efficent model using the noisy students weights. We will use the features and fine tune it with our dataset.\n",
    "### we do not have enough data for training it from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.9.0-dev20220113\n",
      "Num GPUs: 1\n",
      "Num CPUs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-26 16:53:32.890297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-26 16:53:32.900227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-26 16:53:32.900799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "## NO HUB IMPORT PLEASE USE THIS WITH TENSORFLOW 2.xx nightly\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import cv2\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pylab as plt\n",
    "import tensorboard\n",
    "SEEDS = 42\n",
    "np.random.seed(SEEDS)\n",
    "tf.random.set_seed(SEEDS)\n",
    "import itertools\n",
    "print(\"TF version:\", tf.__version__)\n",
    "GPU = tf.config.list_physical_devices('GPU')\n",
    "CPU= tf.config.list_physical_devices('CPU')\n",
    "print(\"Num GPUs:\", len(GPU))\n",
    "\n",
    "print(\"Num CPUs:\", len(CPU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the memory growth incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs,\n"
     ]
    }
   ],
   "source": [
    "## The logical GPUs is not supported in the tensflow available binary, you need to compile it from scratch for multiple logical GPUs\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "      tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "      print(len(gpus), \"Physical GPUs,\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining training dataset diredtories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_1= \"/home/pms/PMS-dataset/data-set/10-class/crop/training\"\n",
    "data_dir_2= \"/home/pms/PMS-dataset/data-set/10-class/output/training\"\n",
    "\n",
    "data_dir_1 = pathlib.Path(data_dir_1)\n",
    "data_dir_2 = pathlib.Path(data_dir_2)\n",
    "print(\"Training-Directory-combined: \",data_dir_1)\n",
    "print(\"Training-Directory-only-road: \",data_dir_1)\n",
    "\n",
    "\n",
    "image_count_1 = len(list(data_dir_1.glob('*/*.jpg')))\n",
    "image_count_2 = len(list(data_dir_2.glob('*/*.jpg')))\n",
    "print(image_count_1)\n",
    "print(image_count_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing dataset from the directory into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 512\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "BATCH_SIZE = 8\n",
    "TOTAL_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_1 = tf.keras.utils.image_dataset_from_directory(\n",
    "  str(data_dir_1),\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=SEEDS,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_ds_2 = tf.keras.utils.image_dataset_from_directory(\n",
    "  str(data_dir_2),\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=SEEDS,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "val_ds_1 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  str(data_dir_1),\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=SEEDS,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds_2 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  str(data_dir_2),\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=SEEDS,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "class_names = np.array(train_ds_1.class_names)\n",
    "print(class_names)\n",
    "class_names = np.array(train_ds_2.class_names)\n",
    "print(class_names)\n",
    "class_names_val = np.array(val_ds_1.class_names)\n",
    "print(class_names_val)\n",
    "class_names_val = np.array(val_ds_2.class_names)\n",
    "print(class_names_val)\n",
    "#concatenate the datasets\n",
    "train_ds = train_ds_1.concatenate(train_ds_2)\n",
    "val_ds = val_ds_1.concatenate(val_ds_2)\n",
    "\n",
    "train_ds = train_ds_1.concatenate(train_ds_2)\n",
    "val_ds = val_ds_1.concatenate(val_ds_2)\n",
    "\n",
    "print(len(list(train_ds_1.as_numpy_iterator())))\n",
    "print(len(list(train_ds_2.as_numpy_iterator())))\n",
    "print(len(list(train_ds.as_numpy_iterator())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Data augmentation to the training set and rescaling the images to 0-1 in both training and validation set and autotune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.RandomFlip('vertical'),\n",
    "  tf.keras.layers.RandomRotation(2),\n",
    "  tf.keras.layers.RandomZoom(.5),\n",
    "  tf.keras.layers.RandomContrast(0.1, 0.2)\n",
    "])\n",
    "# apply normalization and augmentation to the training set and validation set\n",
    "############################################################################\n",
    "#normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "#train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "#val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "############################################################################\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# check the batch size\n",
    "image_batch, label_batch = next(iter(train_ds.take(1)))\n",
    "print(image_batch.shape, label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1.2])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,10.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for debugging purpose\n",
    "#tf.debugging.set_log_device_placement(True) # this line is to turn on debugging information\n",
    "\n",
    "train_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\", patience=3, restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1) # Enable histogram computation for every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications import EfficientNetV2L\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "base_learning_rate = 0.005\n",
    "\n",
    "def build_model(num_classes,base_learning_rate):\n",
    "    inputs = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "    \n",
    "    model = EfficientNetV2L(include_top=False,input_tensor=inputs, weights=\"imagenet\")\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(TOTAL_CLASSES, activation=\"softmax\", name=\"pred\")(x)\n",
    "    #outputs = layers.Dense(TOTAL_CLASSES, name=\"pred\")(x) # DEFAULT ACTIVATION IS LINEAR\n",
    "    # Compile\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNetV2L\")\n",
    "\n",
    "    model.compile(\n",
    "      optimizer = tf.keras.optimizers.SGD(learning_rate=base_learning_rate,momentum=0.9), \n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(num_classes=TOTAL_CLASSES,base_learning_rate=base_learning_rate)\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the transfer learning model\n",
    "epochs = 5 # @param {type: \"slider\", min:8, max:80}\n",
    "# class weight are calculated by the total of samples divided by the number of classes and the number of samples in each class\n",
    "class_weights = {0: 0.85,1 : 0.83,2 : 1.495,3 : 1.57,4 : 2.153,5 : 2.53,6 : 1.17,7 : 0.89,8 : 0.609,9 : 0.54}\n",
    "history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=tensorboard_callback,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##for fine tuning we keep a accurate optimizer like SGD and slow learning rate\n",
    "learning_rate = 0.0005\n",
    "def unfreeze_model(model,learning_rate):\n",
    "    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers[-20:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    model.compile(\n",
    "    optimizer = optimizer, \n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "unfreeze_model(model,learning_rate)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training the fine tuned model\n",
    "NUM_EPOCHS = 50\n",
    "# class weight are calculated by the total of samples divided by the number of classes and the number of samples in each class\n",
    "class_weights = {0: 0.85,1 : 0.83,2 : 1.495,3 : 1.57,4 : 2.153,5 : 2.53,6 : 1.17,7 : 0.89,8 : 0.609,9 : 0.54}\n",
    "history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=tensorboard_callback,\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = model\n",
    "with tf.device('device:GPU:0'):\n",
    "  test_data_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/10-class/output/testing')\n",
    "  print(test_data_dir)\n",
    "  img_height = 512\n",
    "  img_width = 512\n",
    "  batch_size = 8\n",
    "  class_number = 10\n",
    "\n",
    "  test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_data_dir,\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "   ## NO NEED for normalization here \n",
    "  \n",
    "  #test_dataset = test_dataset.map(lambda x, y: (data_augmentation (x), y)) # Where x—images, y—labels.\n",
    "  ds = train_ds.concatenate(val_ds).concatenate(test_dataset)\n",
    "  loss, accuracy = reloaded.evaluate(test_dataset)\n",
    "  print('Test accuracy :', accuracy)\n",
    "  print('Test loss :', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/pms/PMS-dataset/data-set/10-class/combined/testing\n",
    "Found 292 files belonging to 10 classes.\n",
    "37/37 [==============================] - 12s 329ms/step - loss: 4.3840 - accuracy: 0.2842\n",
    "37/37 [==============================] - 12s 329ms/step - loss: 5.3218 - accuracy: 0.2808\n",
    "37/37 [==============================] - 12s 332ms/step - loss: 4.7976 - accuracy: 0.3185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('pms_model/efficientnetv2l-finetune-imagenet-512-new')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation accuracy : 0.6344085931777954\n",
    "Training accuracy : 0.9576897025108337\n",
    "Test accuracy : 0.3116438388824463 292\n",
    "Total accuracy : 0.8142127394676208\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt\n",
    "from xlwt import Workbook\n",
    "  \n",
    "# Workbook is created\n",
    "wb = Workbook()\n",
    "# add_sheet is used to create sheet.\n",
    "test = wb.add_sheet('Test-Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('device:GPU:0'):\n",
    "    result_test_data_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/10-class/output/testing')\n",
    "    output_test_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/10-class/output/results/output_combineModel-ev2l-Keras-finetunning-ImgNetWeight-512')\n",
    "    try:\n",
    "        os.makedirs(output_test_dir, exist_ok=True)\n",
    "        print(\"Directory created\")\n",
    "    except OSError as e:\n",
    "        print(\"Directory already exists\")\n",
    "count = 0\n",
    "countf = 0\n",
    "countF = 0\n",
    "countT = 0\n",
    "count_loop = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "count4 = 0\n",
    "count5 = 0\n",
    "count6 = 0\n",
    "count7 = 0\n",
    "count8 = 0\n",
    "count9 = 0\n",
    "count10 = 0\n",
    "\n",
    "root, class_n, files = next(os.walk(result_test_data_dir))\n",
    "print(class_n)\n",
    "for c in class_n:\n",
    "    new_root = os.path.join(output_test_dir, c)\n",
    "    try:\n",
    "        os.mkdir(new_root)\n",
    "    except OSError as e:\n",
    "        print(\"Directory already exists\")\n",
    "    for dirs in os.walk(os.path.join(root, c)):\n",
    "        for files in dirs:\n",
    "            for filename in files:\n",
    "                if(filename.endswith(\".jpg\")):\n",
    "                    path = os.path.join(root, c, filename)\n",
    "                    #img = tf.keras.utils.load_img(path, target_size=(img_height, img_width))\n",
    "                    orig = cv2.imread(path)\n",
    "                    img = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, (img_height, img_width))\n",
    "                    img_array = tf.keras.utils.img_to_array(img)\n",
    "                    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "                    ##NO Need of normalization here\n",
    "                    ##img_array = normalization_layer(img_array)\n",
    "                    \n",
    "                    with tf.device('device:GPU:0'):\n",
    "                        predictions = reloaded.predict(img_array)\n",
    "                        score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "                    if (int(class_names[np.argmax(score)])==int(c)):\n",
    "                        count=count+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==10):\n",
    "                        countf=countf+1\n",
    "                        count10=count10+1\n",
    "                    elif(int(class_names[np.argmax(score)])==9):\n",
    "                        countf=countf+1\n",
    "                        count9=count9+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==8):\n",
    "                        countf=countf+1\n",
    "                        count8=count8+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==7):\n",
    "                        countf=countf+1\n",
    "                        count7=count7+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==6):\n",
    "                        countf=countf+1\n",
    "                        count6=count6+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==5):\n",
    "                        countf=countf+1\n",
    "                        count5=count5+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==4):\n",
    "                        countf=countf+1\n",
    "                        count4=count4+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==3):\n",
    "                        countf=countf+1\n",
    "                        count3=count3+1\n",
    "                    elif(int(class_names[np.argmax(score)])==2):\n",
    "                        countf=countf+1\n",
    "                        count2=count2+1\n",
    "                    elif(int(class_names[np.argmax(score)])==1):\n",
    "                        countf=countf+1\n",
    "                        count1=count1+1\n",
    "                    else:\n",
    "                        print('error')\n",
    "                    new_filename = filename.split(\".\")[0]+ filename.split(\".\")[1] + \"_\" + \"P_\" + str(int(class_names[np.argmax(score)])) + \".jpg\"\n",
    "                    new_path = os.path.join(new_root, new_filename)\n",
    "                    cv2.imwrite(new_path, orig)\n",
    "                    #print(new_filename )\n",
    "    countT = countT + count\n",
    "    countF = countF + countf\n",
    "    print('=========================================================')\n",
    "    print('class: ',c, 'Images: ',len(files), 'TP: ', count/len(files), 'FP: ',countf/len(files))\n",
    "    #print('one : ',count1/len(files),'two: ',count2/len(files),'three: ',count3/len(files),'four: ',count4/len(files),'five: ',count5/len(files),'six: ',count6/len(files),'seven: ',count7/len(files),'eight: ',count8/len(files),'nine: ',count9/len(files),'ten: ',count10/len(files))\n",
    "    print('one: ',count1,'two: ',count2,'three: ',count3,'four: ',count4,'five: ',count5,'six: ',count6,'seven: ',count7,'eight: ',count8,'nine: ',count9,'ten: ',count10)\n",
    "    test.write(int(c), 1, count1)\n",
    "    test.write(int(c), 2, count2)\n",
    "    test.write(int(c), 3, count3)\n",
    "    test.write(int(c), 4, count4)\n",
    "    test.write(int(c), 5, count5)\n",
    "    test.write(int(c), 6, count6)\n",
    "    test.write(int(c), 7, count7)\n",
    "    test.write(int(c), 8, count8)\n",
    "    test.write(int(c), 9, count9)\n",
    "    test.write(int(c), 10, count10)\n",
    "    test.write(int(c), 11, count)\n",
    "    test.write(int(c), 12, countf)\n",
    "\n",
    "    count = 0\n",
    "    countf = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count5 = 0\n",
    "    count6 = 0\n",
    "    count7 = 0\n",
    "    count8 = 0\n",
    "    count9 = 0\n",
    "    count10 = 0\n",
    "    count_loop = count_loop + 1\n",
    "print('Total classes: ',count_loop,'Total TP: ', countT/(countT+countF), 'Total FP: ',countF)\n",
    "test.write(11, 0, countT/(countT+countF))\n",
    "test.write(11, 1, countF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb.save('output_road__combined-road-Model-testData-ev2l-Keras-finetunning-ImgNetWeight-512.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrain the outer layers with more data from only road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workbook is created\n",
    "wb2 = Workbook()\n",
    "# add_sheet is used to create sheet.\n",
    "test2 = wb2.add_sheet('Test-Summary-road')\n",
    "test2.write(0,0,'No')\n",
    "test2.write(0,1,'filename')\n",
    "test2.write(0,2,'true-class')\n",
    "test2.write(0,3,'predicted-class')\n",
    "iter = 1\n",
    "\n",
    "with tf.device('device:GPU:0'):\n",
    "    #result_test_data_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/10-class/combined/testing')\n",
    "    result_test_data_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/10-class/output/testing')\n",
    "    #output_test_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/10-class/combined/results/test')\n",
    "    output_test_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/10-class/output/results/test')\n",
    "    try:\n",
    "        os.makedirs(output_test_dir, exist_ok=True)\n",
    "        print(\"Directory created\")\n",
    "    except OSError as e:\n",
    "        print(\"Directory already exists\")\n",
    "count = 0\n",
    "countf = 0\n",
    "countF = 0\n",
    "countT = 0\n",
    "count_loop = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "count4 = 0\n",
    "count5 = 0\n",
    "\n",
    "root, class_n, files = next(os.walk(result_test_data_dir))\n",
    "print(class_n)\n",
    "for c in class_n:\n",
    "    new_root = os.path.join(output_test_dir, c)\n",
    "    try:\n",
    "        os.mkdir(new_root)\n",
    "    except OSError as e:\n",
    "        print(\"Directory already exists\")\n",
    "    for dirs in os.walk(os.path.join(root, c)):\n",
    "        for files in dirs:\n",
    "            for filename in files:\n",
    "                if(filename.endswith(\".jpg\")):\n",
    "                    path = os.path.join(root, c, filename)\n",
    "                    #img = tf.keras.utils.load_img(path, target_size=(img_height, img_width))\n",
    "                    orig = cv2.imread(path)\n",
    "                    img = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, (img_height, img_width))\n",
    "                    img_array = tf.keras.utils.img_to_array(img)\n",
    "                    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "                    #-------------------------------\n",
    "                    ##NO NORMALIZATION REQUIRED\n",
    "                    ##img_array = normalization_layer(img_array)\n",
    "                    #-------------------------------\n",
    "                    predictions = reloaded.predict(img_array)\n",
    "                    score = tf.nn.softmax(predictions[0])\n",
    "                    ## write to excel------------------------------------\n",
    "                    test2.write(iter, 0, iter)\n",
    "                    test2.write(iter, 1, filename)\n",
    "                    test2.write(iter, 2, int(c))\n",
    "                    test2.write(iter, 3, int(class_names[np.argmax(score)]))\n",
    "                    iter = iter + 1 # this iterator is for serial number in excel file to count number of jpg files\n",
    "                        #-----------------------------------------------------\n",
    "                    if (int(class_names[np.argmax(score)])==int(c)):\n",
    "                        count=count+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==5):\n",
    "                        countf=countf+1\n",
    "                        count5=count5+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==4):\n",
    "                        countf=countf+1\n",
    "                        count4=count4+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==3):\n",
    "                        countf=countf+1\n",
    "                        count3=count3+1\n",
    "                    elif(int(class_names[np.argmax(score)])==2):\n",
    "                        countf=countf+1\n",
    "                        count2=count2+1\n",
    "                    elif(int(class_names[np.argmax(score)])==1):\n",
    "                        countf=countf+1\n",
    "                        count1=count1+1\n",
    "                    elif(int(class_names[np.argmax(score)])==6):\n",
    "                        countf=countf+1\n",
    "                        count6=count6+1\n",
    "                    elif(int(class_names[np.argmax(score)])==7):\n",
    "                        countf=countf+1\n",
    "                        count7=count7+1\n",
    "                    elif(int(class_names[np.argmax(score)])==8):\n",
    "                        countf=countf+1\n",
    "                        count8=count8+1\n",
    "                    elif(int(class_names[np.argmax(score)])==9):\n",
    "                        countf=countf+1\n",
    "                        count9=count9+1\n",
    "                    elif(int(class_names[np.argmax(score)])==10):\n",
    "                        countf=countf+1\n",
    "                        count10=count10+1\n",
    "                    else:\n",
    "                        print('error')\n",
    "                    new_filename = filename.split(\".\")[0]+ filename.split(\".\")[1] + \"_\" + \"P_\" + str(int(class_names[np.argmax(score)])) + \".jpg\"\n",
    "                    new_path = os.path.join(new_root, new_filename)\n",
    "                    cv2.imwrite(new_path, orig)\n",
    "                    #print(new_filename )\n",
    "\n",
    "    countT = countT + count\n",
    "    countF = countF + countf\n",
    "    print('=========================================================')\n",
    "    print('class: ',c, 'Images: ',len(files), 'TP: ', count/len(files), 'FP: ',countf/len(files))\n",
    "    print('one: ',count1,'two: ',count2,'three: ',count3,'four: ',count4,'five: ',count5)\n",
    "\n",
    "    count = 0\n",
    "    countf = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count5 = 0\n",
    "    count6 = 0\n",
    "    count7 = 0\n",
    "    count8 = 0\n",
    "    count9 = 0\n",
    "    count10 = 0\n",
    "    count_loop = count_loop + 1\n",
    "print('Total classes: ',count_loop,'Total TP: ', countT/(countT+countF), 'Total FP: ',countF)\n",
    "\n",
    "wb2.save('road-10-classes-finetune.xls')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae85714aaa4675f82ee9f7fc78406fe72191dfc57a9d1ac25f5187021de0688d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('waqar-tf-latest': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
