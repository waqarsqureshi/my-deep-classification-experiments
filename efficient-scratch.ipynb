{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This note book runs the efficent model using the noisy students weights. We will use the features and fine tune it with our dataset.\n",
    "### we do not have enough data for training it from scratch\n",
    "\n",
    "Remember whenever learning from scractch for EfficientNet\n",
    "Note: each Keras Application expects a specific kind of input preprocessing. For EfficientNet, input preprocessing is included as part of the model (as a Rescaling layer), and thus tf.keras.applications.efficientnet.preprocess_input is actually a pass-through function. EfficientNet models expect their inputs to be float tensors of pixels with values in the [0-255] range.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import cv2\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pylab as plt\n",
    "import tensorboard\n",
    "SEEDS = 42\n",
    "np.random.seed(SEEDS)\n",
    "tf.random.set_seed(SEEDS)\n",
    "import itertools\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "GPU = tf.config.list_physical_devices('GPU')\n",
    "CPU= tf.config.list_physical_devices('CPU')\n",
    "print(\"Num GPUs:\", len(GPU))\n",
    "\n",
    "print(\"Num CPUs:\", len(CPU))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the memory growth incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The logical GPUs is not supported in the tensflow available binary, you need to compile it from scratch for multiple logical GPUs\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "      tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "      print(len(gpus), \"Physical GPUs,\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining training dataset diredtories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/pms/PMS-dataset/data-set/combined/training\"\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "print(\"Training-Directory: \",data_dir)\n",
    "\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing dataset from the directory into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 600\n",
    "IMAGE_HEIGHT = 600\n",
    "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "BATCH_SIZE = 1\n",
    "TOTAL_CLASSES = 10\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  str(data_dir),\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=SEEDS,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  str(data_dir),\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=SEEDS,\n",
    "  image_size=IMAGE_SIZE,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(4):\n",
    "    ax = plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "class_names = np.array(train_ds.class_names)\n",
    "print(class_names)\n",
    "class_names_val = np.array(val_ds.class_names)\n",
    "print(class_names_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Data augmentation to the training set and rescaling the images to 0-1 in both training and validation set and autotune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.RandomFlip('vertical'),\n",
    "  tf.keras.layers.RandomRotation(2),\n",
    "  tf.keras.layers.RandomZoom(.5),\n",
    "  tf.keras.layers.RandomContrast(0.1, 0.2)\n",
    "])\n",
    "# apply normalization and augmentation to the training set and validation set\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "## WARNING: the rescaling layer is not applied if uisng keras EfficientNet\n",
    "\n",
    "#train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "#val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# check the batch size\n",
    "image_batch, label_batch = next(iter(train_ds.take(1)))\n",
    "print(image_batch.shape, label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for debugging purpose\n",
    "#tf.debugging.set_log_device_placement(True) # this line is to turn on debugging information\n",
    "\n",
    "train_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\", patience=3, restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1) # Enable histogram computation for every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "x = (inputs)\n",
    "outputs = EfficientNetB7(include_top=True, weights=None, classes=TOTAL_CLASSES)(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "base_learning_rate = 0.001\n",
    "## choose the optimizer uncomment the one you want\n",
    "\n",
    "#model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "#              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(learning_rate=base_learning_rate, momentum=0.9), \n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "class_weights = {0: 0.85,1 : 0.83,2 : 1.495,3 : 1.57,4 : 2.153,5 : 2.53,6 : 1.17,7 : 0.89,8 : 0.609,9 : 0.54}\n",
    "\n",
    "history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=tensorboard_callback,\n",
    "        \n",
    "        class_weight=class_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,3.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_hist(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = model\n",
    "with tf.device('device:GPU:0'):\n",
    "  test_data_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/combined/testing')\n",
    "  print(test_data_dir)\n",
    "  img_height = 224\n",
    "  img_width = 224\n",
    "  batch_size = 1\n",
    "  class_number = 10\n",
    "\n",
    "  test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_data_dir,\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)\n",
    "  #test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
    "\n",
    "  loss, accuracy = reloaded.evaluate(test_dataset)\n",
    "  print('Test accuracy :', accuracy)\n",
    "  print('Test loss :', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('device:GPU:0'):\n",
    "    result_test_data_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/combined/testing')\n",
    "    output_test_dir = pathlib.Path('/home/pms/PMS-dataset/data-set/combined/output_test-efficientnetB7-scratch-3epochs-batch-size-1')\n",
    "    try:\n",
    "        os.makedirs(output_test_dir, exist_ok=True)\n",
    "        print(\"Directory created\")\n",
    "    except OSError as e:\n",
    "        print(\"Directory already exists\")\n",
    "count = 0\n",
    "countf = 0\n",
    "countF = 0\n",
    "countT = 0\n",
    "count_loop = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "count4 = 0\n",
    "count5 = 0\n",
    "count6 = 0\n",
    "count7 = 0\n",
    "count8 = 0\n",
    "count9 = 0\n",
    "count10 = 0\n",
    "\n",
    "root, class_n, files = next(os.walk(result_test_data_dir))\n",
    "print(class_n)\n",
    "for c in class_n:\n",
    "    new_root = os.path.join(output_test_dir, c)\n",
    "    try:\n",
    "        os.mkdir(new_root)\n",
    "    except OSError as e:\n",
    "        print(\"Directory already exists\")\n",
    "    for dirs in os.walk(os.path.join(root, c)):\n",
    "        for files in dirs:\n",
    "            for filename in files:\n",
    "                if(filename.endswith(\".jpg\")):\n",
    "                    path = os.path.join(root, c, filename)\n",
    "                    #img = tf.keras.utils.load_img(path, target_size=(img_height, img_width))\n",
    "                    orig = cv2.imread(path)\n",
    "                    img = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, (img_height, img_width))\n",
    "                    img_array = tf.keras.utils.img_to_array(img)\n",
    "                    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "                    #img_array = normalization_layer(img_array)\n",
    "                    with tf.device('device:GPU:0'):\n",
    "                        predictions = reloaded.predict(img_array)\n",
    "                        score = tf.nn.softmax(predictions[0])\n",
    "                        y_pred = tf.math.argmax(score)\n",
    "                        y_pred = int(y_pred)\n",
    "                    if (int(class_names[np.argmax(score)])==int(c)):\n",
    "                        count=count+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==10):\n",
    "                        countf=countf+1\n",
    "                        count10=count10+1\n",
    "                    elif(int(class_names[np.argmax(score)])==9):\n",
    "                        countf=countf+1\n",
    "                        count9=count9+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==8):\n",
    "                        countf=countf+1\n",
    "                        count8=count8+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==7):\n",
    "                        countf=countf+1\n",
    "                        count7=count7+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==6):\n",
    "                        countf=countf+1\n",
    "                        count6=count6+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==5):\n",
    "                        countf=countf+1\n",
    "                        count5=count5+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==4):\n",
    "                        countf=countf+1\n",
    "                        count4=count4+1\n",
    "                    elif(int(class_names[np.argmax(score)]) ==3):\n",
    "                        countf=countf+1\n",
    "                        count3=count3+1\n",
    "                    elif(int(class_names[np.argmax(score)])==2):\n",
    "                        countf=countf+1\n",
    "                        count2=count2+1\n",
    "                    elif(int(class_names[np.argmax(score)])==1):\n",
    "                        countf=countf+1\n",
    "                        count1=count1+1\n",
    "                    else:\n",
    "                        print('error')\n",
    "                    new_filename = filename.split(\".\")[0]+ filename.split(\".\")[1] + \"_\" + \"P_\" + str(int(class_names[np.argmax(score)])) + \".jpg\"\n",
    "                    new_path = os.path.join(new_root, new_filename)\n",
    "                    cv2.imwrite(new_path, orig)\n",
    "                    #print(new_filename )\n",
    "    countT = countT + count\n",
    "    countF = countF + countf\n",
    "    print('=========================================================')\n",
    "    print('class: ',c, 'Images: ',len(files), 'TP: ', count/len(files), 'FP: ',countf/len(files))\n",
    "    #print('one : ',count1/len(files),'two: ',count2/len(files),'three: ',count3/len(files),'four: ',count4/len(files),'five: ',count5/len(files),'six: ',count6/len(files),'seven: ',count7/len(files),'eight: ',count8/len(files),'nine: ',count9/len(files),'ten: ',count10/len(files))\n",
    "    print('one: ',count1,'two: ',count2,'three: ',count3,'four: ',count4,'five: ',count5,'six: ',count6,'seven: ',count7,'eight: ',count8,'nine: ',count9,'ten: ',count10)\n",
    "    test.write(int(c), 1, count1)\n",
    "    test.write(int(c), 2, count2)\n",
    "    test.write(int(c), 3, count3)\n",
    "    test.write(int(c), 4, count4)\n",
    "    test.write(int(c), 5, count5)\n",
    "    test.write(int(c), 6, count6)\n",
    "    test.write(int(c), 7, count7)\n",
    "    test.write(int(c), 8, count8)\n",
    "    test.write(int(c), 9, count9)\n",
    "    test.write(int(c), 10, count10)\n",
    "    test.write(int(c), 11, count)\n",
    "    test.write(int(c), 12, countf)\n",
    "\n",
    "    count = 0\n",
    "    countf = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    count5 = 0\n",
    "    count6 = 0\n",
    "    count7 = 0\n",
    "    count8 = 0\n",
    "    count9 = 0\n",
    "    count10 = 0\n",
    "    count_loop = count_loop + 1\n",
    "print('Total classes: ',count_loop,'Total TP: ', countT/(countT+countF), 'Total FP: ',countF)\n",
    "test.write(11, 0, countT/(countT+countF))\n",
    "test.write(11, 1, countF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb.save('test-efficientnetB7-with-weight-scratch.xls')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae85714aaa4675f82ee9f7fc78406fe72191dfc57a9d1ac25f5187021de0688d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('waqar-tf-latest': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
